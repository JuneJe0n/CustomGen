"""
Use face lmks, face mesh to align face
Extract HED from aligned face & pose
"""
import argparse, cv2, torch, numpy as np
from pathlib import Path
from PIL import Image, ImageFilter
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
from diffusers.models.controlnets.multicontrolnet import MultiControlNetModel
from controlnet_aux import HEDdetector
from insightface.app import FaceAnalysis
from ip_adapter import IPAdapterXL
from skimage.transform import SimilarityTransform, warp
import mediapipe as mp

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT = "a baby sitting, clear facial features, detailed, realistic, smooth colors"
NEG = "(lowres, bad quality, watermark, disjointed, strange limbs, cut off, bad anatomymissing limbs, fused fingers)"
FACE_IMG  = Path("/data2/jeesoo/FFHQ/64000/64000.png")
POSE_IMG  = Path("/data2/jiyoon/custom/data/pose/p2.jpeg")
STYLE_IMG = Path("/data2/jiyoon/custom/data/style/s3.png")

CN_HED     = "/data2/jiyoon/custom/ckpts/controlnet-union-sdxl-1.0"
BASE_SDXL  = "stabilityai/stable-diffusion-xl-base-1.0"
STYLE_ENC  = "/data2/jiyoon/IP-Adapter/sdxl_models/image_encoder"
STYLE_IP   = "/data2/jiyoon/IP-Adapter/sdxl_models/ip-adapter_sdxl.bin"

COND_HED     = 0.8
STYLE_SCALE  = 0.8
CFG, STEPS   = 7.0, 50
SEED         = 42
OUTDIR       = Path("/data2/jiyoon/custom/results/warp/64000")
OUTDIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_rgb(p): return Image.open(p).convert("RGB")

def to_sdxl_res(img, base=64, short=1024, long=1024):
    w, h = img.size
    r = short / min(w, h); w, h = int(w*r), int(h*r)
    r = long  / max(w, h); w, h = int(w*r), int(h*r)
    return img.resize(((w//base)*base, (h//base)*base), Image.LANCZOS)

def align_face_with_landmarks(face_img, pose_img, face_det):
    """InsightFace ëœë“œë§ˆí¬ë¥¼ ì´ìš©í•œ ì–¼êµ´ ì •ë ¬"""
    
    face_cv = cv2.cvtColor(np.array(face_img), cv2.COLOR_RGB2BGR)
    pose_cv = cv2.cvtColor(np.array(pose_img), cv2.COLOR_RGB2BGR)
    
    # ì–¼êµ´ ì •ë³´ ì¶”ì¶œ
    face_infos = face_det.get(face_cv)
    pose_infos = face_det.get(pose_cv)
    
    if not face_infos or not pose_infos:
        print("âš ï¸  ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ë²• ì‚¬ìš©")
        return face_img, None
    
    # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
    face_info = max(face_infos, key=lambda d: (d['bbox'][2]-d['bbox'][0])*(d['bbox'][3]-d['bbox'][1]))
    pose_info = max(pose_infos, key=lambda d: (d['bbox'][2]-d['bbox'][0])*(d['bbox'][3]-d['bbox'][1]))
    
    # 5ê°œ í•µì‹¬ ëœë“œë§ˆí¬ ì¶”ì¶œ
    face_kps = face_info['kps']  # (5, 2)
    pose_kps = pose_info['kps']
    
    try:
        # Similarity transform ê³„ì‚° (rotation, scaling, translation)
        tform = SimilarityTransform()
        tform.estimate(face_kps, pose_kps)
        
        # face ì´ë¯¸ì§€ë¥¼ poseì— ë§ê²Œ ë³€í˜•
        h, w = pose_img.size[::-1]
        aligned_face = warp(
            np.array(face_img), 
            tform.inverse, 
            output_shape=(h, w),
            preserve_range=True
        )
        
        aligned_face_pil = Image.fromarray(aligned_face.astype(np.uint8))
        
        # ë³€í˜•ëœ ì–¼êµ´ì˜ ìƒˆë¡œìš´ bbox ê³„ì‚°
        transformed_kps = tform(face_kps)
        x_coords = transformed_kps[:, 0]
        y_coords = transformed_kps[:, 1]
        
        margin = 30  # bbox í™•ì¥
        new_bbox = [
            max(0, int(x_coords.min() - margin)),
            max(0, int(y_coords.min() - margin)),
            min(w, int(x_coords.max() + margin)),
            min(h, int(y_coords.max() + margin))
        ]
        
        return aligned_face_pil, new_bbox
        
    except Exception as e:
        print(f"âš ï¸  ëœë“œë§ˆí¬ ì •ë ¬ ì‹¤íŒ¨: {e}, ê¸°ì¡´ ë°©ë²• ì‚¬ìš©")
        return face_img, None

def create_mediapipe_face_mask(img, save_path=None):
    """MediaPipe Face Meshë¡œ ì •ë°€í•œ ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±"""
    
    mp_face_mesh = mp.solutions.face_mesh
    
    with mp_face_mesh.FaceMesh(
        static_image_mode=True,
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5
    ) as face_mesh:
        
        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        results = face_mesh.process(img_cv)
        
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0]
            
            # ì–¼êµ´ ì™¸ê³½ì„  ì¸ë±ìŠ¤ (FACEMESH_FACE_OVAL)
            face_oval = [
                10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,
                397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,
                172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109
            ]
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = np.zeros(img_cv.shape[:2], dtype=np.float32)
            points = []
            
            for idx in face_oval:
                x = int(landmarks.landmark[idx].x * img_cv.shape[1])
                y = int(landmarks.landmark[idx].y * img_cv.shape[0])
                points.append([x, y])
            
            cv2.fillPoly(mask, [np.array(points)], 1.0)
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¶€ë“œëŸ½ê²Œ
            mask = cv2.GaussianBlur(mask, (15, 15), sigmaX=5, sigmaY=5)
            
            # ë§ˆìŠ¤í¬ ì‹œê°í™” ì €ì¥
            if save_path:
                mask_vis = (mask * 255).astype(np.uint8)
                mask_pil = Image.fromarray(mask_vis, mode='L')
                mask_pil.save(save_path)
                print(f"ğŸ’¾ MediaPipe ë§ˆìŠ¤í¬ ì €ì¥: {save_path}")
            
            return mask[..., None]  # (H, W, 1)
    
    return None

def create_enhanced_soft_mask(pose_img, bbox, save_dir=None):
    """í–¥ìƒëœ ì†Œí”„íŠ¸ ë§ˆìŠ¤í¬ ìƒì„±"""
    
    h, w = pose_img.size[::-1]
    x1, y1, x2, y2 = bbox
    
    # MediaPipeë¡œ ì •ë°€í•œ ì–¼êµ´ ë§ˆìŠ¤í¬ ì‹œë„
    mp_mask_path = save_dir / "mediapipe_mask.png" if save_dir else None
    mp_mask = create_mediapipe_face_mask(pose_img, mp_mask_path)
    
    if mp_mask is not None:
        # ì–¼êµ´ ì˜ì—­ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        roi_mask = np.zeros_like(mp_mask)
        roi_mask[y1:y2, x1:x2] = mp_mask[y1:y2, x1:x2]
        return roi_mask
    
    # MediaPipe ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ê°œì„ ëœ ë²„ì „)
    print("âš ï¸  MediaPipe ë§ˆìŠ¤í¬ ìƒì„± ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
    mask = np.zeros((h, w), dtype=np.float32)
    mask[y1:y2, x1:x2] = 1.0
    
    # ë” ë¶€ë“œëŸ¬ìš´ ë¸”ëŸ¬ë§
    blur_size = max(31, int((x2-x1) * 0.1))  # ì–¼êµ´ í¬ê¸°ì— ë¹„ë¡€
    if blur_size % 2 == 0:
        blur_size += 1
    
    mask = cv2.GaussianBlur(mask, (blur_size, blur_size), sigmaX=blur_size//3, sigmaY=blur_size//3)
    
    return mask[..., None]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ (ê°œì„ ëœ ë²„ì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(use_style, gpu_idx):
    DEVICE = f"cuda:{gpu_idx}"
    DTYPE  = torch.float16
    torch.manual_seed(SEED)

    # â”€â”€â”€â”€â”€ ì–¼êµ´ ê°ì§€ & HED detector
    face_det = FaceAnalysis(
        name="antelopev2",
        root="/data2/jiyoon/InstantID",
        providers=[('CUDAExecutionProvider', {'device_id': gpu_idx}), 'CPUExecutionProvider']
    )
    face_det.prepare(ctx_id=gpu_idx, det_size=(640, 640), det_thresh=0.3)
    hed = HEDdetector.from_pretrained("lllyasviel/Annotators").to(DEVICE)

    # â”€â”€â”€â”€â”€ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
    face_im = to_sdxl_res(load_rgb(FACE_IMG))
    pose_im = to_sdxl_res(load_rgb(POSE_IMG))
    style_pil = load_rgb(STYLE_IMG)
    w_pose, h_pose = pose_im.size

    print("ğŸ”„ ëœë“œë§ˆí¬ ê¸°ë°˜ ì–¼êµ´ ì •ë ¬ ì‚¬ìš©")
    print(f"Face image size: {face_im.size}")
    print(f"Pose image size: {pose_im.size}")
    # â”€â”€â”€â”€â”€ ëœë“œë§ˆí¬ ê¸°ë°˜ ì–¼êµ´ ì •ë ¬
    aligned_face, aligned_bbox = align_face_with_landmarks(face_im, pose_im, face_det)
    
    if aligned_bbox is None:
        # ëœë“œë§ˆí¬ ì •ë ¬ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ë²• ì‚¬ìš©
        print("ğŸ“¦ ê¸°ì¡´ bbox ë°©ë²• ì‚¬ìš©")
        pose_cv = cv2.cvtColor(np.array(pose_im), cv2.COLOR_RGB2BGR)
        p_info = max(face_det.get(pose_cv), key=lambda d:(d['bbox'][2]-d['bbox'][0])*(d['bbox'][3]-d['bbox'][1]))
        x1, y1, x2, y2 = map(int, p_info['bbox'])
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w_pose, x2), min(h_pose, y2)
        
        # face HED ì¶”ì¶œ 
        face_cv = cv2.cvtColor(np.array(face_im), cv2.COLOR_RGB2BGR)
        f_info = max(face_det.get(face_cv), key=lambda d:(d['bbox'][2]-d['bbox'][0])*(d['bbox'][3]-d['bbox'][1]))
        fx1, fy1, fx2, fy2 = map(int, f_info['bbox'])
        face_crop = face_im.crop((fx1, fy1, fx2, fy2))
        face_hed = hed(face_crop, safe=False, scribble=False)
        face_hed_resized = face_hed.resize((x2-x1, y2-y1), Image.LANCZOS)
        face_hed_np = np.array(face_hed_resized).astype(np.float32)
        
    else:
        print("âœ¨ ëœë“œë§ˆí¬ ì •ë ¬ ì„±ê³µ")
        x1, y1, x2, y2 = aligned_bbox
        
        # ì •ë ¬ëœ ì–¼êµ´ì—ì„œ HED ì¶”ì¶œ
        face_crop = aligned_face.crop((x1, y1, x2, y2))
        face_hed = hed(face_crop, safe=False, scribble=False)
        
        # í¬ê¸°ë¥¼ target regionì— ë§ê²Œ ì¡°ì •
        target_w, target_h = x2 - x1, y2 - y1
        face_hed_resized = face_hed.resize((target_w, target_h), Image.LANCZOS)
        face_hed_np = np.array(face_hed_resized).astype(np.float32)

    # â”€â”€â”€â”€â”€ pose ì „ì²´ HED
    pose_hed_pil = hed(pose_im, safe=False, scribble=False).resize(pose_im.size, Image.LANCZOS)
    pose_hed_np = np.array(pose_hed_pil).astype(np.float32)

    # â”€â”€â”€â”€â”€ í–¥ìƒëœ ì†Œí”„íŠ¸ ë§ˆìŠ¤í‚¹ (MediaPipe ë§ˆìŠ¤í¬ ì €ì¥)
    mask = create_enhanced_soft_mask(pose_im, (x1, y1, x2, y2), save_dir=OUTDIR)

    # face HEDë¥¼ ì „ì²´ canvasì— ë°°ì¹˜
    face_canvas_np = np.zeros_like(pose_hed_np).astype(np.float32)
    face_canvas_np[y1:y2, x1:x2] = face_hed_np

    # ì†Œí”„íŠ¸ ë¸”ë Œë”©
    pose_np = pose_hed_np.astype(np.float32)
    blended_np = mask * face_canvas_np + (1 - mask) * pose_np
    blended_np = blended_np.clip(0, 255).astype(np.uint8)
    merged_hed_pil = Image.fromarray(blended_np).convert("RGB")
    
    # ê²°ê³¼ ì €ì¥
    merged_hed_pil.save(OUTDIR/"merged_hed_enhanced.png")
    print(f"ğŸ’¾ HED ì €ì¥: {OUTDIR/'merged_hed_enhanced.png'}")

    # â”€â”€â”€â”€â”€ ControlNet êµ¬ì„±
    controlnets, images, scales, masks = [], [], [], []
    controlnets.append(ControlNetModel.from_pretrained(CN_HED, torch_dtype=DTYPE))
    images.append(merged_hed_pil)
    scales.append(COND_HED)
    masks.append(None)

    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
        BASE_SDXL,
        controlnet=MultiControlNetModel(controlnets),
        torch_dtype=DTYPE,
        add_watermarker=False
    ).to(DEVICE)

    pipe.enable_vae_tiling()
    pipe.enable_xformers_memory_efficient_attention()

    if not use_style:
        pipe.enable_sequential_cpu_offload()
    else:
        pipe.to(DEVICE)

    # â”€â”€â”€â”€â”€ ì´ë¯¸ì§€ ìƒì„±
    gen_args = dict(
        prompt=PROMPT,
        negative_prompt=NEG,
        num_inference_steps=STEPS,
        guidance_scale=CFG,
        image=images,
        controlnet_conditioning_scale=scales,
        control_mask=masks,
    )

    if use_style:
        ip = IPAdapterXL(
            pipe, STYLE_ENC, STYLE_IP, DEVICE,
            target_blocks=["up_blocks.0.attentions.1"]
        )
        out = ip.generate(pil_image=style_pil, scale=STYLE_SCALE,
                          seed=SEED, **gen_args)[0]
    else:
        out = pipe(**gen_args).images[0]

    fname = OUTDIR/"enhanced_landmark.png"
    out.save(fname)
    print(f"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥: {fname}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--style", action="store_true", help="IP-Adapter ìŠ¤íƒ€ì¼ ì£¼ì…")
    ap.add_argument("--gpu", type=int, default=0, help="GPU ë²ˆí˜¸")
    args = ap.parse_args()
    main(args.style, args.gpu)