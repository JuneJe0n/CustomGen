"""
ê°œì„ ëœ ë²„ì „: pose imageì—ì„œëŠ” pose keypointsë§Œ, face imageì—ì„œëŠ” face alignment ì‚¬ìš©
"""
import argparse, cv2, torch, numpy as np
from pathlib import Path
from PIL import Image, ImageFilter
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
from diffusers.models.controlnets.multicontrolnet import MultiControlNetModel
from controlnet_aux import HEDdetector, OpenposeDetector
from insightface.app import FaceAnalysis
from ip_adapter import IPAdapterXL
from skimage.transform import SimilarityTransform, warp
import mediapipe as mp

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT = "a baby sitting, clear facial features, detailed, realistic, smooth colors"
NEG = "(lowres, bad quality, watermark, disjointed, strange limbs, cut off, bad anatomymissing limbs, fused fingers)"
FACE_IMG  = Path("/data2/jeesoo/FFHQ/10000/10000.png")
POSE_IMG  = Path("/data2/jiyoon/custom/data/pose/p2.jpeg")
STYLE_IMG = Path("/data2/jiyoon/custom/data/style/s3.png")

CN_HED     = "/data2/jiyoon/custom/ckpts/controlnet-union-sdxl-1.0"
CN_POSE    = "/data2/jiyoon/custom/ckpts/controlnet-openpose-sdxl-1.0"  # pose controlnet ì¶”ê°€
BASE_SDXL  = "stabilityai/stable-diffusion-xl-base-1.0"
STYLE_ENC  = "/data2/jiyoon/IP-Adapter/sdxl_models/image_encoder"
STYLE_IP   = "/data2/jiyoon/IP-Adapter/sdxl_models/ip-adapter_sdxl.bin"

COND_HED     = 0.8
COND_POSE    = 0.6  # pose conditioning strength
STYLE_SCALE  = 0.8
CFG, STEPS   = 7.0, 50
SEED         = 42
OUTDIR       = Path("/data2/jiyoon/custom/results/warp/10000")
OUTDIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_rgb(p): return Image.open(p).convert("RGB")

def to_sdxl_res(img, base=64, short=1024, long=1024):
    w, h = img.size
    r = short / min(w, h); w, h = int(w*r), int(h*r)
    r = long  / max(w, h); w, h = int(w*r), int(h*r)
    return img.resize(((w//base)*base, (h//base)*base), Image.LANCZOS)

def extract_pose_keypoints(pose_img, pose_detector):
    """Pose imageì—ì„œ pose keypointsë§Œ ì¶”ì¶œ"""
    print("ğŸ¤¸ Pose keypoints ì¶”ì¶œ ì¤‘...")
    pose_kps = pose_detector(pose_img)
    
    # ê²°ê³¼ ì €ì¥
    pose_kps.save(OUTDIR / "pose_keypoints.png")
    print(f"ğŸ’¾ Pose keypoints ì €ì¥: {OUTDIR / 'pose_keypoints.png'}")
    
    return pose_kps

def align_face_for_hed(face_img, target_size=(512, 512)):
    """Face imageì—ì„œë§Œ face alignment ìˆ˜í–‰í•˜ì—¬ HED ì¶”ì¶œ"""
    print("ğŸ‘¤ Face alignment ë° HED ì¶”ì¶œ ì¤‘...")
    
    # ì–¼êµ´ ê°ì§€
    face_cv = cv2.cvtColor(np.array(face_img), cv2.COLOR_RGB2BGR)
    face_infos = face_det.get(face_cv)
    
    if not face_infos:
        print("âš ï¸  ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨")
        return None, None
    
    # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
    face_info = max(face_infos, key=lambda d: (d['bbox'][2]-d['bbox'][0])*(d['bbox'][3]-d['bbox'][1]))
    bbox = face_info['bbox']
    kps = face_info['kps']
    
    # ì–¼êµ´ ì˜ì—­ crop ë° ì •ë ¬
    x1, y1, x2, y2 = map(int, bbox)
    face_crop = face_img.crop((x1, y1, x2, y2))
    
    # í‘œì¤€ ì–¼êµ´ landmarks (ì •ë©´ ì–¼êµ´ ê¸°ì¤€)
    standard_kps = np.array([
        [0.31556875000000000, 0.4615741071428571],  # left eye
        [0.68262291666666670, 0.4615741071428571],  # right eye  
        [0.50026249999999990, 0.6405053571428571],  # nose
        [0.34947187500000004, 0.8246919642857142],  # left mouth
        [0.65343645833333330, 0.8246919642857142]   # right mouth
    ]) * np.array(target_size)
    
    try:
        # cropëœ ì–¼êµ´ì—ì„œì˜ ìƒëŒ€ì  keypoints
        crop_kps = kps - np.array([x1, y1])
        
        # Similarity transform ê³„ì‚°
        tform = SimilarityTransform()
        tform.estimate(crop_kps, standard_kps)
        
        # ì–¼êµ´ ì •ë ¬
        aligned_face = warp(
            np.array(face_crop),
            tform.inverse,
            output_shape=target_size,
            preserve_range=True
        )
        
        aligned_face_pil = Image.fromarray(aligned_face.astype(np.uint8))
        
        # HED ì¶”ì¶œ
        face_hed = hed(aligned_face_pil, safe=False, scribble=False)
        
        # ê²°ê³¼ ì €ì¥
        aligned_face_pil.save(OUTDIR / "aligned_face.png")
        face_hed.save(OUTDIR / "face_hed.png")
        print(f"ğŸ’¾ ì •ë ¬ëœ ì–¼êµ´ ì €ì¥: {OUTDIR / 'aligned_face.png'}")
        print(f"ğŸ’¾ Face HED ì €ì¥: {OUTDIR / 'face_hed.png'}")
        
        return aligned_face_pil, face_hed
        
    except Exception as e:
        print(f"âš ï¸  Face alignment ì‹¤íŒ¨: {e}")
        return None, None

def create_face_region_mask(pose_img, face_det, save_path=None):
    """Pose imageì—ì„œ ì–¼êµ´ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„± (HED ë¸”ë Œë”©ìš©)"""
    
    # MediaPipeë¡œ ì •ë°€í•œ ì–¼êµ´ ë§ˆìŠ¤í¬ ì‹œë„
    mp_mask = create_mediapipe_face_mask(pose_img, save_path)
    
    if mp_mask is not None:
        return mp_mask
    
    # MediaPipe ì‹¤íŒ¨ì‹œ InsightFace bbox ì‚¬ìš©
    print("âš ï¸  MediaPipe ì‹¤íŒ¨, InsightFace bbox ì‚¬ìš©")
    pose_cv = cv2.cvtColor(np.array(pose_img), cv2.COLOR_RGB2BGR)
    pose_infos = face_det.get(pose_cv)
    
    if not pose_infos:
        print("âš ï¸  ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨")
        return None
    
    face_info = max(pose_infos, key=lambda d: (d['bbox'][2]-d['bbox'][0])*(d['bbox'][3]-d['bbox'][1]))
    x1, y1, x2, y2 = map(int, face_info['bbox'])
    
    # ë¶€ë“œëŸ¬ìš´ ë§ˆìŠ¤í¬ ìƒì„±
    h, w = pose_img.size[::-1]
    mask = np.zeros((h, w), dtype=np.float32)
    mask[y1:y2, x1:x2] = 1.0
    
    # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬
    blur_size = max(31, int((x2-x1) * 0.15))
    if blur_size % 2 == 0:
        blur_size += 1
    
    mask = cv2.GaussianBlur(mask, (blur_size, blur_size), sigmaX=blur_size//3, sigmaY=blur_size//3)
    
    return mask[..., None]

def create_mediapipe_face_mask(img, save_path=None):
    """MediaPipe Face Meshë¡œ ì •ë°€í•œ ì–¼êµ´ ë§ˆìŠ¤í¬ ìƒì„±"""
    
    mp_face_mesh = mp.solutions.face_mesh
    
    with mp_face_mesh.FaceMesh(
        static_image_mode=True,
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5
    ) as face_mesh:
        
        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        results = face_mesh.process(img_cv)
        
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0]
            
            # ì–¼êµ´ ì™¸ê³½ì„  ì¸ë±ìŠ¤
            face_oval = [
                10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,
                397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,
                172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109
            ]
            
            # ë§ˆìŠ¤í¬ ìƒì„±
            mask = np.zeros(img_cv.shape[:2], dtype=np.float32)
            points = []
            
            for idx in face_oval:
                x = int(landmarks.landmark[idx].x * img_cv.shape[1])
                y = int(landmarks.landmark[idx].y * img_cv.shape[0])
                points.append([x, y])
            
            cv2.fillPoly(mask, [np.array(points)], 1.0)
            
            # ë¶€ë“œëŸ¬ìš´ ë¸”ëŸ¬
            mask = cv2.GaussianBlur(mask, (21, 21), sigmaX=7, sigmaY=7)
            
            if save_path:
                mask_vis = (mask * 255).astype(np.uint8)
                Image.fromarray(mask_vis, mode='L').save(save_path)
                print(f"ğŸ’¾ Face mask ì €ì¥: {save_path}")
            
            return mask[..., None]
    
    return None

def blend_face_hed_with_pose(face_hed, pose_img, face_mask):
    """Face HEDë¥¼ pose image ì–¼êµ´ ì˜ì—­ì—ë§Œ ë¸”ë Œë”©"""
    
    # pose imageì™€ ê°™ì€ í¬ê¸°ë¡œ face HED ë¦¬ì‚¬ì´ì¦ˆ
    face_hed_resized = face_hed.resize(pose_img.size, Image.LANCZOS)
    
    # numpy ë°°ì—´ë¡œ ë³€í™˜
    face_hed_np = np.array(face_hed_resized).astype(np.float32)
    pose_np = np.array(pose_img).astype(np.float32)
    
    # ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë Œë”©
    if len(face_hed_np.shape) == 3 and len(face_mask.shape) == 3:
        blended_np = face_mask * face_hed_np + (1 - face_mask) * pose_np
    else:
        # ì±„ë„ ì°¨ì› ë§ì¶”ê¸°
        if len(face_mask.shape) == 3 and face_mask.shape[2] == 1:
            face_mask = np.repeat(face_mask, 3, axis=2)
        blended_np = face_mask * face_hed_np + (1 - face_mask) * pose_np
    
    blended_np = blended_np.clip(0, 255).astype(np.uint8)
    blended_pil = Image.fromarray(blended_np)
    
    # ê²°ê³¼ ì €ì¥
    blended_pil.save(OUTDIR / "blended_face_pose.png")
    print(f"ğŸ’¾ ë¸”ë Œë”© ê²°ê³¼ ì €ì¥: {OUTDIR / 'blended_face_pose.png'}")
    
    return blended_pil

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ (ê°œì„ ëœ ë²„ì „) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(use_style, gpu_idx):
    global face_det, hed
    
    DEVICE = f"cuda:{gpu_idx}"
    DTYPE  = torch.float16
    torch.manual_seed(SEED)

    # â”€â”€â”€â”€â”€ ëª¨ë¸ ë¡œë“œ
    face_det = FaceAnalysis(
        name="antelopev2",
        root="/data2/jiyoon/InstantID",
        providers=[('CUDAExecutionProvider', {'device_id': gpu_idx}), 'CPUExecutionProvider']
    )
    face_det.prepare(ctx_id=gpu_idx, det_size=(640, 640))
    
    hed = HEDdetector.from_pretrained("lllyasviel/Annotators").to(DEVICE)
    openpose = OpenposeDetector.from_pretrained("lllyasviel/Annotators").to(DEVICE)

    # â”€â”€â”€â”€â”€ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
    face_im = to_sdxl_res(load_rgb(FACE_IMG))
    pose_im = to_sdxl_res(load_rgb(POSE_IMG))
    style_pil = load_rgb(STYLE_IMG)

    print("ğŸ”„ ê°œì„ ëœ íŒŒì´í”„ë¼ì¸: Face alignment + Pose keypoints")
    
    # â”€â”€â”€â”€â”€ 1. Pose imageì—ì„œ keypoints ì¶”ì¶œ
    pose_keypoints = extract_pose_keypoints(pose_im, openpose)
    
    # â”€â”€â”€â”€â”€ 2. Face imageì—ì„œ ì •ë ¬ëœ HED ì¶”ì¶œ
    aligned_face, face_hed = align_face_for_hed(face_im, target_size=(512, 512))
    
    if aligned_face is None or face_hed is None:
        print("âŒ Face alignment ì‹¤íŒ¨")
        return
    
    # â”€â”€â”€â”€â”€ 3. Pose imageì—ì„œ ì–¼êµ´ ì˜ì—­ ë§ˆìŠ¤í¬ ìƒì„±
    face_mask = create_face_region_mask(
        pose_im, 
        face_det, 
        save_path=OUTDIR / "face_region_mask.png"
    )
    
    if face_mask is None:
        print("âŒ Face mask ìƒì„± ì‹¤íŒ¨")
        return
    
    # â”€â”€â”€â”€â”€ 4. Face HEDë¥¼ pose image ì–¼êµ´ ì˜ì—­ì— ë¸”ë Œë”©
    blended_result = blend_face_hed_with_pose(face_hed, pose_im, face_mask)
    
    # â”€â”€â”€â”€â”€ 5. Multi-ControlNet êµ¬ì„±
    controlnets = [
        ControlNetModel.from_pretrained(CN_POSE, torch_dtype=DTYPE),  # pose
        ControlNetModel.from_pretrained(CN_HED, torch_dtype=DTYPE)    # face HED
    ]
    
    control_images = [pose_keypoints, blended_result]
    control_scales = [COND_POSE, COND_HED]

    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
        BASE_SDXL,
        controlnet=MultiControlNetModel(controlnets),
        torch_dtype=DTYPE,
        add_watermarker=False
    ).to(DEVICE)

    pipe.enable_vae_tiling()
    pipe.enable_xformers_memory_efficient_attention()

    if not use_style:
        pipe.enable_sequential_cpu_offload()

    # â”€â”€â”€â”€â”€ 6. ì´ë¯¸ì§€ ìƒì„±
    gen_args = dict(
        prompt=PROMPT,
        negative_prompt=NEG,
        num_inference_steps=STEPS,
        guidance_scale=CFG,
        image=control_images,
        controlnet_conditioning_scale=control_scales,
    )

    if use_style:
        ip = IPAdapterXL(
            pipe, STYLE_ENC, STYLE_IP, DEVICE,
            target_blocks=["up_blocks.0.attentions.1"]
        )
        out = ip.generate(pil_image=style_pil, scale=STYLE_SCALE,
                          seed=SEED, **gen_args)[0]
    else:
        out = pipe(**gen_args).images[0]

    fname = OUTDIR / "result_face_pose_separated.png"
    out.save(fname)
    print(f"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥: {fname}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--style", action="store_true", help="IP-Adapter ìŠ¤íƒ€ì¼ ì£¼ì…")
    ap.add_argument("--gpu", type=int, default=0, help="GPU ë²ˆí˜¸")
    args = ap.parse_args()
    main(args.style, args.gpu)